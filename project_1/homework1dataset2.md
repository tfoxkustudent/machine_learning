# Roses are Red
This project is the the second data set where I used my kmeans algorithm to cluster data and find centroids of data. We picked the iris data set from kaggle to use the algorithm on. I picked the y-axis to be petal length and the x-axis to be petal width. These are measurements of the flower. I used different colors to cluster the different species of iris. Go into terminal and run command python3 flower_clusters.py You will see four different boxes pop up. The top two boxes are the data points plotted, one with robot moves in color and the other with non-colored data. This gives you an opportunity to see the data points plotted. The bottom two graphs show the kmeans algorithm vs the actual known iris species.

# How the K Means Algorithm works:
So first we initialize the kmeans class with our training data and the number of clusters we want. Then we randomize the number of data points and use the first number of points to randomly place our centroids. We then iterate through every data point and calculate the distance for every centroid. we then pick the minimum distance and determine the nearest centroid and store that into an array. We then go through the associated data points to each centroid and calculate the total mean for those data points, then we are able to place the centroid based on that calculation. We then interate through this process a certain amount of times to achieve the minimum distance for our clusters and their associated data points.

# K-means optimization
The way to optimize K-means is to optimize the number of clusters selected. Each data set has a total sum of squares (TSS) of vector norms from the data set's mean. As clusters are used, the residual sum of squares (RSS) of the subsets norms from the cluster mean can be used to judge a convergence limit. This technique is based on the "elbow" method as described in an external reference. The algorithm keeps adding to K until the RSS/TSS reaches a lower threshold. At this point the algorithm evaluates each K's Calinski-Harabasz index to determine the optimal K which occurs at the maximum Calinski-Harabasz score. Since the Calinski-Harabasz index seeks to minimize the amount of clusters as well as RSS of the data, this seeks a good middle ground for determining the optimal number of clusters. 

# Conclusion
This is fairly good correlation with the different species types and leaf dimensions. This data set was great for visualizing how well our algorithm works in practice.  THe data set gives us three fairly distinct clusters of data, when using petal lenght and width to determine the type of specie of flower.  Our algorithm does a fantastic job clustering the data and determining the amount of clusters needed for the data.  One of our fears is over-fitting the data.  Here in this example, our data was not over-fit at all.
